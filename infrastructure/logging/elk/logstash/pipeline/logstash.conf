input {
  # Filebeat input
  beats {
    port => 5044
    type => "filebeat"
  }

  # Direct TCP input for applications
  tcp {
    port => 5000
    codec => json_lines
    type => "application"
  }

  # Syslog input
  syslog {
    port => 5514
    type => "syslog"
  }

  # HTTP input for webhooks
  http {
    port => 8080
    codec => json
    type => "webhook"
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^{.*}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Extract service name from container name
  if [container][name] {
    mutate {
      add_field => { 
        "service_name" => "%{[container][name]}"
      }
    }
    
    # Clean up service names
    mutate {
      gsub => [
        "service_name", "-\d+$", "",
        "service_name", "_\d+$", ""
      ]
    }
  }

  # Parse Kong access logs
  if [service_name] == "kong" {
    grok {
      match => { 
        "message" => '%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] "%{WORD:method} %{URIPATH:path}(?:%{URIPARAM:params})? HTTP/%{NUMBER:http_version}" %{NUMBER:status} %{NUMBER:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}" %{NUMBER:request_time} %{NUMBER:upstream_response_time} %{IPORHOST:upstream_host}:%{INT:upstream_port}'
      }
    }
    
    mutate {
      convert => {
        "status" => "integer"
        "body_bytes_sent" => "integer"
        "request_time" => "float"
        "upstream_response_time" => "float"
      }
    }
  }

  # Parse application logs
  if [parsed][level] {
    mutate {
      add_field => { 
        "log_level" => "%{[parsed][level]}"
      }
    }
  }

  # Extract error details
  if [parsed][error] {
    mutate {
      add_field => {
        "error_message" => "%{[parsed][error][message]}"
        "error_stack" => "%{[parsed][error][stack]}"
      }
    }
  }

  # Add timestamp
  if [parsed][timestamp] {
    date {
      match => [ "[parsed][timestamp]", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Environment enrichment
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:development}"
      "cluster" => "ecommerce-cluster"
    }
  }

  # GeoIP enrichment for client IPs
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}" ]
    }
  }

  # Performance metrics extraction
  if [parsed][metrics] {
    ruby {
      code => '
        metrics = event.get("[parsed][metrics]")
        if metrics.is_a?(Hash)
          metrics.each do |key, value|
            event.set("metric_#{key}", value)
          end
        end
      '
    }
  }

  # Tag slow requests
  if [request_time] {
    if [request_time] > 1.0 {
      mutate {
        add_tag => [ "slow_request" ]
      }
    }
  }

  # Tag errors
  if [status] {
    if [status] >= 400 and [status] < 500 {
      mutate {
        add_tag => [ "client_error" ]
      }
    } else if [status] >= 500 {
      mutate {
        add_tag => [ "server_error" ]
      }
    }
  }

  # Remove parsed field after extraction
  mutate {
    remove_field => [ "parsed", "message" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ecommerce-logs-%{+YYYY.MM.dd}"
    template_name => "ecommerce-logs"
    template => "/usr/share/logstash/templates/ecommerce-logs.json"
    template_overwrite => true
  }

  # Output errors to separate index
  if "error" in [tags] or "server_error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecommerce-errors-%{+YYYY.MM.dd}"
    }
  }

  # Output slow requests to separate index
  if "slow_request" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecommerce-performance-%{+YYYY.MM.dd}"
    }
  }

  # Debug output (disable in production)
  # stdout {
  #   codec => rubydebug
  # }
}